{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Books Sales Trend "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to study book data, including the number of reviews and the books that is listed as a bestseller, to uncover key trends such as the most in-demand genres and the factors that attract readers and increase a book’s popularity. This analysis is expected to provide valuable insights that can help publishers and authors enhance their marketing strategies and boost the success of their books. \n",
    "\n",
    "#### The analysis will focus on understanding:\n",
    "\n",
    "- How do ratings and the number of reviews vary among bestsellers?\n",
    "- Are certain authors more likely to have their books become bestsellers?\n",
    "- Does the attractiveness of a book's cover influence its likelihood of becoming a bestseller?\n",
    "- What genres are most represented among bestsellers?\n",
    "- What is the relationship between price and bestseller books? , What is the price range of bestseller books?\n",
    "\n",
    "\n",
    "This project is expected to contribute valuable insights to the publishing industry and help stakeholders make data-driven decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Collecting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Data Collection Period: [30 Jan 2025] to [2 Feb 2025]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We collected our data by web scraping from the following online stores:\n",
    "\n",
    "- **Amazon KSA Online Store:** A global platform with a wide range of books, including international bestsellers.\n",
    "\n",
    "- **Jarir KSA Online Store:** A leading bookstore in Saudi Arabia, offering both Arabic and English books.\n",
    "\n",
    "These sources were chosen because they represent a diverse range of books, have a large and diverse audience, and provide relatively complete data. By focusing on bestseller lists, we aim to study the factors that contribute to a book’s success in these markets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To study the factors influencing bestseller books, we identified key attributes that are likely to have a significant impact on a book’s popularity. After reviewing related studies, research papers, and articles, we referenced the following sources to guide our attribute selection [1][2][3].\n",
    "\n",
    "#### Based on these references, we selected the following attributes for our dataset:\n",
    "\n",
    "- **Title:** The name of the book.\n",
    "\n",
    "- **Price:** The retail price of the book.\n",
    "\n",
    "- **Rating:** The average customer rating (e.g., out of 5 stars).\n",
    "\n",
    "- **Num Of Reviews:** The total number of customer reviews.\n",
    "\n",
    "- **Author:** The name of the author(s).\n",
    "\n",
    "- **Book Type:** The format of the book (e.g., paperback, hardcover, eBook).\n",
    "\n",
    "- **Genre:** The category or genre of the book (e.g., fiction, non-fiction, self-help).\n",
    "\n",
    "- **Cover Image:** The image of the book cover (for visual analysis or reference).\n",
    "\n",
    "These attributes were chosen because they are commonly associated with a book’s success and can help answer key questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges in Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Data collection comes with various challenges that can hinder efficiency and accuracy. In our process, which involves web scraping, we faced several key difficulties:\n",
    "1. **Time-Consuming Process:**\n",
    "Data collection, especially when using web scraping techniques, requires significant time due to the complexity of extracting and processing data from multiple sources.\n",
    "\n",
    "3. **Unclear HTML Structure:**\n",
    "Some essential elements like <div> and <span> do not have clear or consistent class names, making it difficult to identify and extract the required data efficiently.\n",
    "\n",
    "5. **Dynamic Content with JavaScript:**\n",
    "Certain websites load content dynamically using JavaScript, which means that the data may not be visible in the initial HTML source code. This requires additional tools or techniques to handle dynamic content effectively.\n",
    "\n",
    "6. **Request Limits and Access Restrictions:**\n",
    "Some data sources impose strict limits on the number of requests that can be made within a specific timeframe, while others require special access permissions or API keys.\n",
    "\n",
    "7. **Inconsistent Data Availability:**\n",
    "Some information is available in certain sources but missing in others, leading to incomplete datasets and making it challenging to ensure data consistency and reliability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions Taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  **Small-Scale Testing Before Full Collection:** We tested the scraping code on a small dataset to ensure accuracy. Once confirmed, we scaled up to collect the full dataset, avoiding repetitive work and saving time. \n",
    "\n",
    "2. **Relied on HTML Attributes and Structure:**\n",
    "To handle unclear or inconsistent class names, we used element IDs or the DOM structure. For elements in arrays with the same class, we relied on their positional consistency to extract data.\n",
    "\n",
    "3. **Used Selenium for Dynamic Content:**\n",
    "For JavaScript-loaded content, we implemented Selenium to interact with pages like a browser, ensuring dynamic content was fully loaded before scraping. \n",
    "\n",
    "4. **Added Time Delays Between Requests:**\n",
    "To avoid being blocked, we introduced time delays between requests to simulate natural user behavior, reducing the risk of triggering rate limits. \n",
    "\n",
    "5. **Leveraged Ready-Made Scraping Tools:**\n",
    "Tools like Instant Data Scraper helped us gather initial data efficiently. We collected links to individual pages and accessed them separately to minimize request limits. \n",
    "\n",
    "6. **Combined Data from Multiple Sources:**\n",
    "To address missing or inconsistent data, we merged datasets from different sources, ensuring a more comprehensive and reliable final dataset. \n",
    "\n",
    "7. **Conducted Manual Reviews and Validation:**\n",
    "We manually reviewed samples of scraped data to identify and correct errors, ensuring high data quality and refining our scripts for better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Web Scraping Tools used:\n",
    "- Web scraper - free web scraping : https://chromewebstore.google.com/detail/web-scraper-free-web-scra/jnhgnonknehpejjnehehllkliplmbmhn?hl=en <br>\n",
    "<b>Used when faced server blocking, in Amazon store <b>\n",
    "- Instant Data Scraper - free web scraping  : https://chromewebstore.google.com/detail/instant-data-scraper/ofaokhiedipichpaobibbnahnkdoiiah <br>\n",
    "<b>Used to extract bestseller books URLs from Jarir <b>\n",
    "\n",
    "####  Libraries, Modules, and Methods Used in the Web Scraping Script\n",
    "-  <b> Libraries Used:<b>\n",
    "1. time → Standard Python library used to add delays between requests.\n",
    "2. pandas → Handles CSV file operations (reading & saving scraped data).\n",
    "3. requests → Fetches HTML content from web pages.\n",
    "bs4 (BeautifulSoup) → Parses and extracts static HTML elements.\n",
    "4. selenium → Automates browser interactions for scraping dynamically loaded content.\n",
    "- <b> Modules Used (from Selenium): <b>\n",
    "1. selenium.webdriver → Controls the Chrome browser for web scraping.\n",
    "2. selenium.webdriver.common.by → Provides mechanisms to locate elements in the HTML (e.g., by class name, ID).\n",
    "3. selenium.webdriver.support.ui → Contains WebDriverWait for handling dynamically loaded elements.\n",
    "4. selenium.webdriver.support.expected_conditions (imported as EC) → Defines conditions for checking if elements are present before interacting with them.\n",
    " - <b> Methods Used: <b>\n",
    "1. time.sleep(seconds) → Pauses execution to allow page elements to load.\n",
    "2. requests.get(url, headers=HEADERS) → Sends an HTTP request to fetch page content.\n",
    "3. BeautifulSoup(response.text, \"html.parser\") → Parses the HTML response for static content.\n",
    "4. driver.get(url) → Loads a webpage using Selenium.\n",
    "5. WebDriverWait(driver, timeout).until(condition) → Waits for a web element to appear before scraping.\n",
    "6. EC.presence_of_element_located((By.CLASS_NAME, \"tf-rating\")) → Checks if an element is present in the DOM.\n",
    "7. soup.find(tag, class_=\"class-name\") → Finds the first occurrence of an element in the HTML.\n",
    "8. soup.find_all(tag, class_=\"class-name\") → Finds all matching elements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazom bestseller books "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have decided to collect data from Amazon's bestseller Books by applying web scraping for two pages ( each one contain around 50 books )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # used to send HTTP requests to web servers\n",
    "from bs4 import BeautifulSoup # parsing HTML and XML documents\n",
    "import pandas as pd # powerful data manipulation and analysis library\n",
    "import numpy as np # used for numerical computations in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pages = 2\n",
    "def get_data(pageNo):\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\", \n",
    "               \"Accept-Encoding\":\"gzip, deflate\", \n",
    "               \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \n",
    "               \"DNT\":\"1\", \"Connection\":\"close\", \n",
    "               \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "\n",
    "    r = requests.get(f'https://www.amazon.sa/-/en/gp/bestsellers/books/ref=zg_bs_pg_1_books?ie=UTF8&pg={pageNo}&language=en&crid=1MSN01VVU9GYY&qid=1711400365&rnid=12463048031&sprefix=engl+book%2Cstripbooks%2C312&ref=sr_pg_{pageNo}', headers=headers)\n",
    "    content = r.content\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    alls = []\n",
    "    for d in soup.findAll('div', attrs={'class':'zg-grid-general-faceout'}): \n",
    "        name = d.find('div', attrs={'class':'_cDEzb_p13n-sc-css-line-clamp-1_1Fn1y'})\n",
    "        price = d.find('span', attrs={'class':'_cDEzb_p13n-sc-price_3mJ9Z'})\n",
    "        rating = d.find('span', attrs={'class':'a-icon-alt'})\n",
    "        users_rated = d.find('span', attrs={'aria-hidden':'true'})\n",
    "        author = d.find('div', attrs={'class':'a-row'})\n",
    "        format_type = d.find('span', attrs={'class':'a-text-normal'})\n",
    "        genre = d.find('div', attrs={'class':'a-row a-size-base a-color-base'})\n",
    "        cover_image = d.find('img', attrs={'class': 'a-dynamic-image p13n-sc-dynamic-image p13n-product-image'})\n",
    "\n",
    "        all1 = []\n",
    "\n",
    "        if name is not None:\n",
    "            all1.append(name.text)\n",
    "        else:\n",
    "            all1.append(\"Null\")\n",
    "\n",
    "        if price is not None:\n",
    "            all1.append(price.text)\n",
    "        else:\n",
    "            all1.append(\"Null\")\n",
    "\n",
    "        if rating is not None:\n",
    "            all1.append(rating.text)\n",
    "        else:\n",
    "            all1.append(\"Null\")\n",
    "\n",
    "        if users_rated is not None:\n",
    "            all1.append(users_rated.text)\n",
    "        else:\n",
    "            all1.append(\"Null\")\n",
    "\n",
    "        if author is not None:\n",
    "            all1.append(author.text)\n",
    "        else:\n",
    "            all1.append(\"Null\")\n",
    "\n",
    "        if format_type is not None:\n",
    "            all1.append(format_type.text)\n",
    "        else:\n",
    "            all1.append(\"Null\")\n",
    "\n",
    "        if genre is not None:\n",
    "            all1.append(genre.text)\n",
    "        else:\n",
    "            all1.append(\"Null\")\n",
    "\n",
    "        if cover_image is not None:\n",
    "            all1.append(cover_image['src'])\n",
    "        else:\n",
    "            all1.append(\"No Image\")\n",
    "\n",
    "        alls.append(all1)\n",
    "    books = soup.findAll('div', attrs={'class': 'zg-grid-general-faceout'})\n",
    "    print(f\"Books found : {len(books)}\")\n",
    "    return alls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books found : 30\n",
      "Books found : 30\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(1, no_pages+1):\n",
    "    results.append(get_data(i))\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "df = pd.DataFrame(flatten(results), columns=[\n",
    "    'Title',          \n",
    "    'Price',         \n",
    "    'Rating',          \n",
    "    'Num Of Reviews', \n",
    "    'Author',         \n",
    "    'Book Type',      \n",
    "    'Genre',     \n",
    "    'Cover Image'     \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We notice here that only 30 books have been extacted from each page out of 50, could be due to server blocking. that's why we decided to collect other books using an extension for web scraping from google chrome.<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Web scraper - free web scraping : https://chromewebstore.google.com/detail/web-scraper-free-web-scra/jnhgnonknehpejjnehehllkliplmbmhn?hl=en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Num Of Reviews</th>\n",
       "      <th>Author</th>\n",
       "      <th>Book Type</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Cover Image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>كتاب التحصيلي علمي 46-47 (2025)</td>\n",
       "      <td>SAR 98.00</td>\n",
       "      <td>4.3 out of 5 stars</td>\n",
       "      <td>10</td>\n",
       "      <td>Nasser bin Abdulaziz Al-Abdulkarim</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Null</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El Sharq library المعاصر 9 تاسيس كمي 2/1 ورقي ...</td>\n",
       "      <td>SAR 107.58</td>\n",
       "      <td>4.5 out of 5 stars</td>\n",
       "      <td>230</td>\n",
       "      <td>عماد الجزيري</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Null</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My First Library : Boxset Of 10 Board Books Fo...</td>\n",
       "      <td>SAR 59.00</td>\n",
       "      <td>4.6 out of 5 stars</td>\n",
       "      <td>80,714</td>\n",
       "      <td>Wonder House Books</td>\n",
       "      <td>Board book</td>\n",
       "      <td>Null</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>فاتتني صلاة</td>\n",
       "      <td>SAR 26.00</td>\n",
       "      <td>4.7 out of 5 stars</td>\n",
       "      <td>301</td>\n",
       "      <td>اسلام جمال</td>\n",
       "      <td>Unknown Binding</td>\n",
       "      <td>Null</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Power of your Subconscious Mind</td>\n",
       "      <td>SAR 35.00</td>\n",
       "      <td>4.5 out of 5 stars</td>\n",
       "      <td>14,131</td>\n",
       "      <td>Joseph Murphy</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Null</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Coloriages mystères Disney Princesses: Colorie...</td>\n",
       "      <td>SAR 109.10</td>\n",
       "      <td>4.7 out of 5 stars</td>\n",
       "      <td>5,880</td>\n",
       "      <td>Jérémy Mariez</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Null</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>101 Unicorn Colouring Book: Fun Activity Colou...</td>\n",
       "      <td>SAR 21.62</td>\n",
       "      <td>4.6 out of 5 stars</td>\n",
       "      <td>2,321</td>\n",
       "      <td>Wonder House Books</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Null</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>White Nights</td>\n",
       "      <td>SAR 19.00</td>\n",
       "      <td>4.6 out of 5 stars</td>\n",
       "      <td>1,583</td>\n",
       "      <td>Fyodor Dostoyevsky</td>\n",
       "      <td>Mass Market Paperback</td>\n",
       "      <td>Null</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Null</td>\n",
       "      <td>SAR 65.00</td>\n",
       "      <td>4.7 out of 5 stars</td>\n",
       "      <td>12,588</td>\n",
       "      <td>4.7 out of 5 stars 12,588</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Null</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>My First 365 Coloring Book: Jumbo Coloring Boo...</td>\n",
       "      <td>SAR 29.00</td>\n",
       "      <td>4.5 out of 5 stars</td>\n",
       "      <td>1,625</td>\n",
       "      <td>Wonder House Books</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Null</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title       Price  \\\n",
       "0                    كتاب التحصيلي علمي 46-47 (2025)   SAR 98.00   \n",
       "1  El Sharq library المعاصر 9 تاسيس كمي 2/1 ورقي ...  SAR 107.58   \n",
       "2  My First Library : Boxset Of 10 Board Books Fo...   SAR 59.00   \n",
       "3                                        فاتتني صلاة   SAR 26.00   \n",
       "4                The Power of your Subconscious Mind   SAR 35.00   \n",
       "5  Coloriages mystères Disney Princesses: Colorie...  SAR 109.10   \n",
       "6  101 Unicorn Colouring Book: Fun Activity Colou...   SAR 21.62   \n",
       "7                                       White Nights   SAR 19.00   \n",
       "8                                               Null   SAR 65.00   \n",
       "9  My First 365 Coloring Book: Jumbo Coloring Boo...   SAR 29.00   \n",
       "\n",
       "               Rating Num Of Reviews                              Author  \\\n",
       "0  4.3 out of 5 stars             10  Nasser bin Abdulaziz Al-Abdulkarim   \n",
       "1  4.5 out of 5 stars            230                        عماد الجزيري   \n",
       "2  4.6 out of 5 stars         80,714                  Wonder House Books   \n",
       "3  4.7 out of 5 stars            301                          اسلام جمال   \n",
       "4  4.5 out of 5 stars         14,131                       Joseph Murphy   \n",
       "5  4.7 out of 5 stars          5,880                       Jérémy Mariez   \n",
       "6  4.6 out of 5 stars          2,321                  Wonder House Books   \n",
       "7  4.6 out of 5 stars          1,583                  Fyodor Dostoyevsky   \n",
       "8  4.7 out of 5 stars         12,588           4.7 out of 5 stars 12,588   \n",
       "9  4.5 out of 5 stars          1,625                  Wonder House Books   \n",
       "\n",
       "               Book Type Genre  \\\n",
       "0              Paperback  Null   \n",
       "1              Paperback  Null   \n",
       "2             Board book  Null   \n",
       "3        Unknown Binding  Null   \n",
       "4              Paperback  Null   \n",
       "5              Paperback  Null   \n",
       "6              Paperback  Null   \n",
       "7  Mass Market Paperback  Null   \n",
       "8              Paperback  Null   \n",
       "9              Paperback  Null   \n",
       "\n",
       "                                         Cover Image  \n",
       "0  https://images-eu.ssl-images-amazon.com/images...  \n",
       "1  https://images-eu.ssl-images-amazon.com/images...  \n",
       "2  https://images-eu.ssl-images-amazon.com/images...  \n",
       "3  https://images-eu.ssl-images-amazon.com/images...  \n",
       "4  https://images-eu.ssl-images-amazon.com/images...  \n",
       "5  https://images-eu.ssl-images-amazon.com/images...  \n",
       "6  https://images-eu.ssl-images-amazon.com/images...  \n",
       "7  https://images-eu.ssl-images-amazon.com/images...  \n",
       "8  https://images-eu.ssl-images-amazon.com/images...  \n",
       "9  https://images-eu.ssl-images-amazon.com/images...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how it looks like \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note: since the genre column is inside each book's page, we had to collect them manually.<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*results might be diffrent since we have collected them few days ago and amazon's bestseller books might have changed a bit*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the data as a csv file\n",
    "df.to_csv(\"amazon_raw_books.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jarir bestseller books "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * *We extrat Arabic and Engligh bestseller books URLs in Jarir (around 260 book) using **Instant Data Scraper** extention and save it in \"jarir_bestsellers.csv\" file to read it and make loop on it.*\n",
    "*  Also we use **selenium** for javaScript dynamic elements like \"Rating\" and \"Num Of Reviews\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "# CSV file containing book URLs\n",
    "df = pd.read_csv(\"jarir_bestsellers.csv\", names=[\"book_link\"])\n",
    "\n",
    "# CSV file containing book URLs\n",
    "INPUT_CSV = \"jarir_bestsellers.csv\"\n",
    "all_books_df = pd.DataFrame()\n",
    "\n",
    "# Headers to mimic a browser request\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0\"\n",
    "}\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def get_book_data(url):\n",
    "    \"\"\"Scrapes book data from Jarir's website.\"\"\"\n",
    "    book_details = {}\n",
    "\n",
    "    # Scrape static content using Requests + BeautifulSoup\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    response.encoding = \"utf-8\"  # Force UTF-8 encoding\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extract Title\n",
    "    book_details[\"Title\"] = soup.find(\"h2\", class_=\"product-title__title\").text.strip() if soup.find(\"h2\", class_=\"product-title__title\") else \"Null\"\n",
    "\n",
    "    # Extract Price\n",
    "    price_container = soup.find(\"span\", class_=\"price_alignment\")\n",
    "    if price_container:\n",
    "        value = price_container.find_all(\"span\")[-1].text.strip() if price_container.find_all(\"span\") else \"Null\"\n",
    "        book_details[\"Price\"] = value\n",
    "    else:\n",
    "        book_details[\"Price\"] = \"Null\"\n",
    "\n",
    "    # Use Selenium for dynamically loaded elements (Rating & Reviews)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Allow time for JavaScript to load\n",
    "\n",
    "    # Extract Rating\n",
    "    try:\n",
    "        rating_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"tf-rating\"))\n",
    "        )\n",
    "        book_details[\"Rating\"] = rating_element.text.strip()\n",
    "    except:\n",
    "        book_details[\"Rating\"] = \"Null\"\n",
    "\n",
    "    # Extract Number of Reviews\n",
    "    try:\n",
    "        num_reviews_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"tf-count\"))\n",
    "        )\n",
    "        book_details[\"Num Of Reviews\"] = num_reviews_element.text.strip()\n",
    "    except:\n",
    "        book_details[\"Num Of Reviews\"] = \"Null\"\n",
    "\n",
    "    # Extract Author\n",
    "    author_tag = soup.find(\"b\", string=\"Author:\")\n",
    "    book_details[\"Author\"] = author_tag.find_next(\"span\", class_=\"cl-blue\").text.strip() if author_tag else \"Null\"\n",
    "\n",
    "    # Extract Book Type (Format)\n",
    "    format_tag = soup.find(\"b\", string=\"Format:\")\n",
    "    book_details[\"Book Type\"] = format_tag.find_next(\"span\").text.strip() if format_tag else \"Null\"\n",
    "\n",
    "    # Extract Genre (Book Classification)\n",
    "    book_classification = soup.find(\"b\", string=\"Book classification:\")\n",
    "    if book_classification:\n",
    "        genres = [span.text.strip() for span in book_classification.find_next(\"span\").find_all(\"span\", class_=\"cl-blue\") if span.text.strip()]\n",
    "        book_details[\"Genre\"] = \", \".join(genres) if genres else \"Null\"\n",
    "    else:\n",
    "        book_details[\"Genre\"] = \"Null\"\n",
    "\n",
    "    # Extract High-Quality Cover Image\n",
    "    image_tags = soup.find_all(\"img\", class_=\"image image--contain\")\n",
    "    if len(image_tags) > 1:\n",
    "        raw_image_url = image_tags[1][\"src\"]\n",
    "        # Modify the URL to get better quality (replace width=54 with width=350)\n",
    "        book_details[\"Cover Image\"] = raw_image_url.replace(\"width=54\", \"width=350\")\n",
    "    else:\n",
    "        book_details[\"Cover Image\"] = \"No Image\"\n",
    "\n",
    "    return book_details\n",
    "\n",
    "\n",
    "# Read URLs from CSV & Scrape Data\n",
    "i=1\n",
    "df = pd.read_csv(INPUT_CSV, names=[\"book_link\"], encoding=\"utf-8\") \n",
    "for url in df[\"book_link\"].dropna():  # Drop NaN values\n",
    "    print(i, f\"Scraping: {url}\")\n",
    "    book_info = get_book_data(url)\n",
    "    i+=1\n",
    "\n",
    "    # Append the scraped data to the DataFrame\n",
    "    all_books_df = pd.concat([all_books_df, pd.DataFrame([book_info])], ignore_index=True)\n",
    "    time.sleep(3)  # Delay to prevent request blocking\n",
    "\n",
    "# Close Selenium WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Drop the URL column (not needed in final output)\n",
    "all_books_df.drop(columns=[\"URL\"], inplace=True, errors=\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books_df.to_csv(\"jarir_raw_books.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "We  found 10 books in Jarir with null \"Rating\" and \"Num Of Reviews\" despite having actual values. These were manually corrected in the CSV file. Other nulls were confirmed to represent zero, as they appeared at the end of the bestseller list. The 10 books, however, were in the first half, surrounded by complete data, justifying manual updates.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### after we conduct Amazon and Jarir bestseller books successfullym we integrate them in one csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the two datasets\n",
    "df_jarir = pd.read_csv(\"jarir_raw_books.csv\", encoding=\"utf-8\")\n",
    "df_amazon = pd.read_csv(\"amazon_raw_books.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Merge (concatenate) them\n",
    "df_merged = pd.concat([df_jarir, df_amazon], ignore_index=True)\n",
    "\n",
    "# Save the merged dataset to a new CSV file\n",
    "merged_filename = \"raw_books.csv\"\n",
    "df_merged.to_csv(merged_filename, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After examining the data, it is clear that some preprocessing and cleaning are needed to make it easier to use, interpret, process, and analyze.\n",
    "\n",
    "We will apply several preprocessing techniques such as normalization, discretization, and other techniques as needed, which will be detailed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "df_jarir = pd.read_csv(\"jarir_raw_books.csv\", encoding=\"utf-8\")\n",
    "df_amazon = pd.read_csv(\"amazon_raw_books.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In our data, some 'Author' values on Jarir dataset are represented as 'n/a', which contain hidden characters. In this code, we want to find these hidden patterns to handle them properly during the preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\\\u200en\\\\u200e/\\\\u200ea\\\\u200e'\""
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the content of row 34 only with hidden characters\n",
    "df_jarir.loc[34, 'Author'].__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hidden_pattern(df):\n",
    "    pattern = '\\u200en\\u200e/\\u200ea\\u200e'\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(\n",
    "            lambda x: 'Unknown' if isinstance(x, str) and pattern in x else x\n",
    "        )\n",
    "    return df\n",
    "\n",
    "# Apply to both DataFrames\n",
    "df_jarir = clean_hidden_pattern(df_jarir)\n",
    "df_amazon = clean_hidden_pattern(df_amazon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Missing values in df_jarir:\n",
      "Title               0\n",
      "Price               0\n",
      "Rating            115\n",
      "Num Of Reviews    115\n",
      "Author             15\n",
      "Book Type           0\n",
      "Genre               0\n",
      "Cover Image         0\n",
      "dtype: int64\n",
      "\n",
      " Missing values in df_amazon:\n",
      "Title             0\n",
      "Price             0\n",
      "Rating            1\n",
      "Num Of Reviews    1\n",
      "Author            1\n",
      "Book Type         0\n",
      "Genre             0\n",
      "Cover Image       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def check_missing_values(df, df_name):\n",
    "    # Find rows containing \"Null\", \"Unknown\", or NaN\n",
    "    missing_rows = df[(df == 'Null').any(axis=1) | \n",
    "                      (df == 'Unknown').any(axis=1) | \n",
    "                      (df.isna()).any(axis=1)]\n",
    "    \n",
    "    missing_counts = ((df == 'Null') | (df == 'Unknown')).sum() + df.isna().sum()\n",
    "    \n",
    "    print(f\"\\n Missing values in {df_name}:\")\n",
    "    print(missing_counts)\n",
    "    \n",
    "    return missing_rows \n",
    "\n",
    "# Apply to both DataFrames\n",
    "missing_jarir = check_missing_values(df_jarir, \"df_jarir\")\n",
    "missing_amazon = check_missing_values(df_amazon, \"df_amazon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with 'Null', 'Unknown', or empty (NaN) values in Jarir dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Num Of Reviews</th>\n",
       "      <th>Author</th>\n",
       "      <th>Book Type</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Cover Image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>365 قصة عن الاخلاق عبرة في كل يوم تبعدك عن الم...</td>\n",
       "      <td>68</td>\n",
       "      <td>4.7</td>\n",
       "      <td>(3 reviews)</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>غلاف مقوى</td>\n",
       "      <td>Jarir Publications Books,, Audio &amp; Video CD/DV...</td>\n",
       "      <td>https://www.jarir.com/cdn-cgi/image/fit=contai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>حكايات الاحلام السعيدة كنز من حكايات الاطفال</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>(2 reviews)</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>غلاف مقوى</td>\n",
       "      <td>Jarir Publications Books,, Children Books,</td>\n",
       "      <td>https://www.jarir.com/cdn-cgi/image/fit=contai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>كنز الاطفال مجموعة من الحكايات الرائعة</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>(1 review)</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>غلاف مقوى</td>\n",
       "      <td>Jarir Publications Books,, Children Books,</td>\n",
       "      <td>https://www.jarir.com/cdn-cgi/image/fit=contai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>مكتبة الطفل الاولى الحيوانات</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>(1 review)</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>كتاب مقوى</td>\n",
       "      <td>Jarir Publications Books,, Children Books,, Be...</td>\n",
       "      <td>https://www.jarir.com/cdn-cgi/image/fit=contai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>مكتبة الطفل الاولى جسمي</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>(1 review)</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>كتاب مقوى</td>\n",
       "      <td>Jarir Publications Books,, Children Books,, Be...</td>\n",
       "      <td>https://www.jarir.com/cdn-cgi/image/fit=contai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>Emily Wilde's Encyclopaedia of Faeries</td>\n",
       "      <td>49</td>\n",
       "      <td>Null</td>\n",
       "      <td>Null</td>\n",
       "      <td>Heather Fawcett</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Fiction &amp; Literature,, New Arrivals,, Best Sel...</td>\n",
       "      <td>https://www.jarir.com/cdn-cgi/image/fit=contai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>The Coworker - You See Her Everyday</td>\n",
       "      <td>59</td>\n",
       "      <td>Null</td>\n",
       "      <td>Null</td>\n",
       "      <td>Freida McFadden</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Fiction &amp; Literature,, Best Sellers,</td>\n",
       "      <td>https://www.jarir.com/cdn-cgi/image/fit=contai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>Psychology of Money - Timeless Lesson on Wealth</td>\n",
       "      <td>79</td>\n",
       "      <td>Null</td>\n",
       "      <td>Null</td>\n",
       "      <td>Morgan Housel</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Business &amp; Management,, Best Sellers</td>\n",
       "      <td>https://www.jarir.com/cdn-cgi/image/fit=contai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>Rich Dad Poor Dad</td>\n",
       "      <td>49</td>\n",
       "      <td>Null</td>\n",
       "      <td>Null</td>\n",
       "      <td>Robert Kiyosaki</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Business &amp; Management,, Best Sellers,</td>\n",
       "      <td>https://www.jarir.com/cdn-cgi/image/fit=contai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>The Final Gambit</td>\n",
       "      <td>55</td>\n",
       "      <td>Null</td>\n",
       "      <td>Null</td>\n",
       "      <td>Jennifer Lynn Barnes</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Best Sellers,, Fiction &amp; Literature</td>\n",
       "      <td>https://www.jarir.com/cdn-cgi/image/fit=contai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  Price Rating  \\\n",
       "34   365 قصة عن الاخلاق عبرة في كل يوم تبعدك عن الم...     68    4.7   \n",
       "50        حكايات الاحلام السعيدة كنز من حكايات الاطفال     49      5   \n",
       "89              كنز الاطفال مجموعة من الحكايات الرائعة     49      5   \n",
       "90                        مكتبة الطفل الاولى الحيوانات     39      5   \n",
       "91                             مكتبة الطفل الاولى جسمي     39      5   \n",
       "..                                                 ...    ...    ...   \n",
       "255             Emily Wilde's Encyclopaedia of Faeries     49   Null   \n",
       "256                The Coworker - You See Her Everyday     59   Null   \n",
       "257    Psychology of Money - Timeless Lesson on Wealth     79   Null   \n",
       "259                                  Rich Dad Poor Dad     49   Null   \n",
       "260                                   The Final Gambit     55   Null   \n",
       "\n",
       "    Num Of Reviews                Author  Book Type  \\\n",
       "34     (3 reviews)               Unknown  غلاف مقوى   \n",
       "50     (2 reviews)               Unknown  غلاف مقوى   \n",
       "89      (1 review)               Unknown  غلاف مقوى   \n",
       "90      (1 review)               Unknown  كتاب مقوى   \n",
       "91      (1 review)               Unknown  كتاب مقوى   \n",
       "..             ...                   ...        ...   \n",
       "255           Null       Heather Fawcett  Paperback   \n",
       "256           Null       Freida McFadden  Paperback   \n",
       "257           Null         Morgan Housel  Paperback   \n",
       "259           Null       Robert Kiyosaki  Paperback   \n",
       "260           Null  Jennifer Lynn Barnes  Paperback   \n",
       "\n",
       "                                                 Genre  \\\n",
       "34   Jarir Publications Books,, Audio & Video CD/DV...   \n",
       "50          Jarir Publications Books,, Children Books,   \n",
       "89          Jarir Publications Books,, Children Books,   \n",
       "90   Jarir Publications Books,, Children Books,, Be...   \n",
       "91   Jarir Publications Books,, Children Books,, Be...   \n",
       "..                                                 ...   \n",
       "255  Fiction & Literature,, New Arrivals,, Best Sel...   \n",
       "256               Fiction & Literature,, Best Sellers,   \n",
       "257               Business & Management,, Best Sellers   \n",
       "259              Business & Management,, Best Sellers,   \n",
       "260                Best Sellers,, Fiction & Literature   \n",
       "\n",
       "                                           Cover Image  \n",
       "34   https://www.jarir.com/cdn-cgi/image/fit=contai...  \n",
       "50   https://www.jarir.com/cdn-cgi/image/fit=contai...  \n",
       "89   https://www.jarir.com/cdn-cgi/image/fit=contai...  \n",
       "90   https://www.jarir.com/cdn-cgi/image/fit=contai...  \n",
       "91   https://www.jarir.com/cdn-cgi/image/fit=contai...  \n",
       "..                                                 ...  \n",
       "255  https://www.jarir.com/cdn-cgi/image/fit=contai...  \n",
       "256  https://www.jarir.com/cdn-cgi/image/fit=contai...  \n",
       "257  https://www.jarir.com/cdn-cgi/image/fit=contai...  \n",
       "259  https://www.jarir.com/cdn-cgi/image/fit=contai...  \n",
       "260  https://www.jarir.com/cdn-cgi/image/fit=contai...  \n",
       "\n",
       "[123 rows x 8 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Rows with 'Null', 'Unknown', or empty (NaN) values in Jarir dataset:\")\n",
    "missing_jarir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with 'Null', 'Unknown', or empty (NaN) values in Amazon dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Num Of Reviews</th>\n",
       "      <th>Author</th>\n",
       "      <th>Book Type</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Cover Image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>مذكرات أخصائي نفسي - نورة محمد الكيال</td>\n",
       "      <td>SARآ 80.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>نورة محمد الكيال</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Psychology &amp; Counseling</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>The Mountain Is You: Transforming Self-Sabotag...</td>\n",
       "      <td>SARآ 65.00</td>\n",
       "      <td>4.7 out of 5 stars</td>\n",
       "      <td>12,555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Motivational Self-Help</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title       Price  \\\n",
       "7               مذكرات أخصائي نفسي - نورة محمد الكيال  SARآ 80.50   \n",
       "22  The Mountain Is You: Transforming Self-Sabotag...  SARآ 65.00   \n",
       "\n",
       "                Rating Num Of Reviews             Author  Book Type  \\\n",
       "7                  NaN            NaN   نورة محمد الكيال  Paperback   \n",
       "22  4.7 out of 5 stars         12,555                NaN  Paperback   \n",
       "\n",
       "                      Genre                                        Cover Image  \n",
       "7   Psychology & Counseling  https://images-eu.ssl-images-amazon.com/images...  \n",
       "22   Motivational Self-Help  https://images-eu.ssl-images-amazon.com/images...  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Rows with 'Null', 'Unknown', or empty (NaN) values in Amazon dataset:\")\n",
    "missing_amazon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " df_jarir after cleaning:\n",
      "  Rating Num Of Reviews              Author\n",
      "0      5   (73 reviews)       ‎وليد العنزي‎\n",
      "1    4.5   (37 reviews)     ‎أدم جيه كيرتز‎\n",
      "2    4.7   (29 reviews)      ‎عماد الجزيري‎\n",
      "3      5   (24 reviews)  ‎روبرت تي كيوساكي‎\n",
      "4      5   (20 reviews)         ‎ادهم فادي‎\n",
      " df_amazon after cleaning:\n",
      "               Rating Num Of Reviews                              Author\n",
      "0  4.3 out of 5 stars              8  Nasser bin Abdulaziz Al-Abdulkarim\n",
      "1  4.5 out of 5 stars            218                        عماد الجزيري\n",
      "2  4.7 out of 5 stars          2,603                       Jérémy Mariez\n",
      "3  4.7 out of 5 stars          5,847                       Jérémy Mariez\n",
      "4  4.6 out of 5 stars          1,618                  Wonder House Books\n"
     ]
    }
   ],
   "source": [
    "rawBooks_df['Rating'] = rawBooks_df['Rating'].replace('Null', float('nan')).fillna(0)\n",
    "rawBooks_df['Num Of Reviews'] = rawBooks_df['Num Of Reviews'].replace('Null', float('nan')).fillna(0)\n",
    "rawBooks_df = rawBooks_df.dropna(subset=['Author'])\n",
    "\n",
    "rawBooks_df[['Rating', 'Num Of Reviews','Author']] \n",
    "def clean_dataframe(df, df_name):\n",
    "\n",
    "    df['Rating'] = df['Rating'].replace('Null', float('nan')).fillna(0)\n",
    "    df['Num Of Reviews'] = df['Num Of Reviews'].replace('Null', float('nan')).fillna(0)\n",
    "\n",
    "    df = df.dropna(subset=['Author'])\n",
    "\n",
    "    print(f\" {df_name} after cleaning:\")\n",
    "    print(df[['Rating', 'Num Of Reviews', 'Author']].head())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply cleaning function to both dataframes\n",
    "df_jarir = clean_dataframe(df_jarir, \"df_jarir\")\n",
    "df_amazon = clean_dataframe(df_amazon, \"df_amazon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- In this section, we will handle missing values in three ways:\n",
    "\n",
    "1. **Unknown Authors**: For rows where the author is listed as \"Unknown\", we will leave it as is. This is because it doesn't represent a truly missing value, but rather indicates that the website did not specify the author's name. Therefore, we will retain \"Unknown\" as it appears on the website.\n",
    "\n",
    "2. **Ratings and Reviews**: Some ratings and reviews are represented as null because they have not been rated. We will replace these null values with zero, indicating that no ratings or reviews were provided.\n",
    "\n",
    "3. **Missing Author Names**: For rows where the author name is null (completely missing), we will drop these entries from the dataset, as they lack essential information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Formatting Numerical Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " df_jarir after cleaning:\n",
      "    Price  Rating  Num Of Reviews\n",
      "0   58.0     5.0              73\n",
      "1   59.0     4.5              37\n",
      "2  120.0     4.7              29\n",
      "3   79.0     5.0              24\n",
      "4   89.0     5.0              20\n",
      "\n",
      " df_amazon after cleaning:\n",
      "     Price  Rating  Num Of Reviews\n",
      "0   98.00     4.3               8\n",
      "1  107.58     4.5             218\n",
      "2  109.10     4.7            2603\n",
      "3  109.10     4.7            5847\n",
      "4   29.00     4.6            1618\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the columns\n",
    "rawBooks_df['Price'] = rawBooks_df['Price'].astype(str).str.extract(r'(\\d+\\.?\\d*)')  # Extracting numeric part from Price\n",
    "rawBooks_df['Rating'] = rawBooks_df['Rating'].astype(str).str.extract(r'(\\d+\\.?\\d*)')  # Extracting numeric part from Rating\n",
    "rawBooks_df['Num Of Reviews'] = rawBooks_df['Num Of Reviews'].astype(str).str.replace(',', '', regex=True) # remove commas from number\n",
    "rawBooks_df['Num Of Reviews'] = rawBooks_df['Num Of Reviews'].str.extract(r'(\\d+)')\n",
    "\n",
    "rawBooks_df['Price'] = rawBooks_df['Price'].astype(float)\n",
    "rawBooks_df['Rating'] = rawBooks_df['Rating'].astype(float)\n",
    "rawBooks_df['Num Of Reviews'] = rawBooks_df['Num Of Reviews'].astype(int)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "rawBooks_df\n",
    "def clean_dataframe(df, df_name):\n",
    "\n",
    "    df['Price'] = df['Price'].astype(str).str.extract(r'(\\d+\\.?\\d*)')  # Extract numeric part from Price \n",
    "    df['Rating'] = df['Rating'].astype(str).str.extract(r'(\\d+\\.?\\d*)')  # Extract numeric part from Rating\n",
    "    df['Num Of Reviews'] = df['Num Of Reviews'].astype(str).str.replace(',', '', regex=True)  # remove commas from number\n",
    "    df['Num Of Reviews'] = df['Num Of Reviews'].str.extract(r'(\\d+)')\n",
    "\n",
    "    df['Price'] = df['Price'].astype(float)\n",
    "    df['Rating'] = df['Rating'].astype(float)\n",
    "    df['Num Of Reviews'] = df['Num Of Reviews'].astype(pd.Int64Dtype())  # Allows NaNs in integer column\n",
    "\n",
    "    print(f\"\\n {df_name} after cleaning:\\n\", df[['Price', 'Rating', 'Num Of Reviews']].head())\n",
    "    return df\n",
    "\n",
    "# Apply cleaning function to both dataframes\n",
    "df_jarir = clean_dataframe(df_jarir, \"df_jarir\")\n",
    "df_amazon = clean_dataframe(df_amazon, \"df_amazon\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - In this section, we clean and standardize three columns: 'Price', 'Rating', and 'Num Of Reviews'. We use regular expressions to extract numeric values from these columns, ensuring only the relevant numbers are retained. After extraction, we convert 'Price' and 'Rating' to the float data type and 'Num Of Reviews' to int, preparing the data for numerical analysis. This process ensures consistency and accuracy, making the dataset suitable for further analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Duplicate Rows and Data Aggregation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "# Clean and normalize text in the 'Title' column (convert to lowercase and remove extra spaces)\n",
    "rawBooks_df['Title Clean'] = rawBooks_df['Title'].str.lower().str.strip()\n",
    "\n",
    "# Identify duplicate rows based on the cleaned 'Title' and 'Author' columns\n",
    "potential_duplicates = rawBooks_df[rawBooks_df.duplicated(subset=['Title Clean', 'Author'], keep=False)]\n",
    "\n",
    "# Print duplicate rows to verify\n",
    "print(\"Duplicate rows before processing:\")\n",
    "print(potential_duplicates)\n",
    "\n",
    "# Aggregate duplicate rows by calculating average and sum for numeric columns, and merging unique values for text columns\n",
    "aggregated_duplicates = potential_duplicates.groupby(\n",
    "    ['Title Clean', 'Author']\n",
    ").agg({\n",
    "    'Price': 'mean',\n",
    "    'Rating': 'mean',\n",
    "    'Num Of Reviews': 'sum',\n",
    "    'Book Type': lambda x: ', '.join(x.unique()),\n",
    "    'Genre': lambda x: ', '.join(x.unique())\n",
    "}).reset_index()\n",
    "\n",
    "# Formatting the numeric columns\n",
    "aggregated_duplicates['Price'] = aggregated_duplicates['Price'].astype(int)  # Remove decimals\n",
    "aggregated_duplicates['Rating'] = aggregated_duplicates['Rating'].round(1)   # Keep one decimal place\n",
    "\n",
    "# Rename the cleaned title column back to 'Title'\n",
    "aggregated_duplicates.rename(columns={'Title Clean': 'Title'}, inplace=True)\n",
    "\n",
    "# Add a cover image link to the aggregated duplicates\n",
    "aggregated_duplicates['Cover Image'] = 'https://images-eu.ssl-images-amazon.com/images/I/81E3hDPr3eL._AC_UL300_SR300,200_.jpg'\n",
    "\n",
    "# Remove duplicate rows from the original DataFrame\n",
    "rawBooks_df.drop_duplicates(subset=['Title Clean', 'Author'], keep=False, inplace=True)\n",
    "\n",
    "# Drop the 'Title Clean' column\n",
    "rawBooks_df.drop(columns='Title Clean', inplace=True)\n",
    "\n",
    "# Merge the aggregated duplicates back into the original DataFrame\n",
    "rawBooks_df = pd.concat([rawBooks_df, aggregated_duplicates], ignore_index=True)\n",
    "\n",
    "# Print the processed rows\n",
    "print(\"\\nProcessed aggregated rows:\")\n",
    "print(tabulate(aggregated_duplicates, headers='keys', tablefmt='grid', showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we normalized the 'Title' column by converting text to lowercase and removing extra spaces to standardize comparisons. Duplicate rows were identified based on the cleaned title and author columns. We then aggregated the duplicates by calculating the mean for numeric columns, summing 'Num Of Reviews', and concatenating unique values in text columns like 'Book Type' and 'Genre'. Finally, the results were displayed in a formatted table using the `tabulate()` library for clear and structured visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We decided to apply normalization to the 'Num Of Reviews' column due to the significant variation in its values. To simplify this, we chose the Min-Max scaling method, as it is the most suitable for the task. By normalizing the data to a scale from 0 to 10, we bring it closer to the 'Rating' column, which has values ranging from 0 to 5. This helps align both columns on a more comparable scale, making them easier to analyze together. To perform the normalization, we used the MinMaxScaler class from the sklearn.preprocessing module, part of the scikit-learn library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_jarir after combined scaling:\n",
      "    Num Of Reviews\n",
      "0         0.00905\n",
      "1         0.00459\n",
      "2         0.00360\n",
      "3         0.00298\n",
      "4         0.00248\n",
      "\n",
      "df_amazon after combined scaling:\n",
      "    Num Of Reviews\n",
      "0         0.00099\n",
      "1         0.02703\n",
      "2         0.32279\n",
      "3         0.72507\n",
      "4         0.20064\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 10))\n",
    "\n",
    "# Combine 'Num Of Reviews' from jarir and amazon\n",
    "combined_df = pd.concat([df_jarir[['Num Of Reviews']], df_amazon[['Num Of Reviews']]], axis=0)\n",
    "\n",
    "scaler.fit(combined_df)\n",
    "\n",
    "df_jarir[['Num Of Reviews']] = scaler.transform(df_jarir[['Num Of Reviews']])\n",
    "df_jarir['Num Of Reviews'] = df_jarir['Num Of Reviews'].round(5)\n",
    "\n",
    "df_amazon[['Num Of Reviews']] = scaler.transform(df_amazon[['Num Of Reviews']])\n",
    "df_amazon['Num Of Reviews'] = df_amazon['Num Of Reviews'].round(5)\n",
    "\n",
    "# Display the scaled values in both dataframes\n",
    "print(\"df_jarir after combined scaling:\\n\", df_jarir[['Num Of Reviews']].head())\n",
    "print(\"\\ndf_amazon after combined scaling:\\n\", df_amazon[['Num Of Reviews']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization and Encoding \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We noticed that the 'Price' column contains continuous values, which need to be processed in a way that makes them simpler to handle. We decided to first perform discretization by dividing the values into five labels using bins. The labels range from 'Very Low' to 'Very High'. After discretization, we applied encoding and mapped these labels ordinally from 0 to 4 to simplify feature processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_jarir Price bins:\n",
      "        Price\n",
      "0       High\n",
      "1       High\n",
      "2  Very High\n",
      "3  Very High\n",
      "4  Very High\n",
      "\n",
      "df_amazon Price bins:\n",
      "        Price\n",
      "0  Very High\n",
      "1  Very High\n",
      "2  Very High\n",
      "3  Very High\n",
      "4        Low\n"
     ]
    }
   ],
   "source": [
    "combined_df = pd.concat([df_jarir[['Price']], df_amazon[['Price']]], axis=0)\n",
    "\n",
    "min_price = combined_df['Price'].min()\n",
    "max_price = combined_df['Price'].max()\n",
    "\n",
    "bin_edges = [min_price,\n",
    "             combined_df['Price'].quantile(0.2),\n",
    "             combined_df['Price'].quantile(0.4),\n",
    "             combined_df['Price'].quantile(0.6),\n",
    "             combined_df['Price'].quantile(0.8),\n",
    "             max_price+1]\n",
    "\n",
    "bin_labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "\n",
    "df_jarir['Price'] = pd.cut(df_jarir['Price'], bins=bin_edges, labels=bin_labels, right=False)\n",
    "\n",
    "df_amazon['Price'] = pd.cut(df_amazon['Price'], bins=bin_edges, labels=bin_labels, right=False)\n",
    "\n",
    "print(\"df_jarir Price bins:\\n\", df_jarir[['Price']].head())\n",
    "print(\"\\ndf_amazon Price bins:\\n\", df_amazon[['Price']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df_jarir Price after encoding:\n",
      "   Price\n",
      "0     3\n",
      "1     3\n",
      "2     4\n",
      "3     4\n",
      "4     4\n",
      "\n",
      "df_amazon Price after encoding:\n",
      "   Price\n",
      "0     4\n",
      "1     4\n",
      "2     4\n",
      "3     4\n",
      "4     1\n"
     ]
    }
   ],
   "source": [
    "# Encode labels to integers\n",
    "label_encoding = {'Very Low': 0, 'Low': 1, 'Medium': 2, 'High': 3, 'Very High': 4}\n",
    "df_jarir['Price'] = df_jarir['Price'].map(label_encoding)\n",
    "df_amazon['Price'] = df_amazon['Price'].map(label_encoding)\n",
    "\n",
    "print(\"\\ndf_jarir Price after encoding:\\n\", df_jarir[['Price']].head())\n",
    "print(\"\\ndf_amazon Price after encoding:\\n\", df_amazon[['Price']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Language Unification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section, we faced a problem with the 'Author' column values. As you can see in our raw data, the names were mixed with Arabic and English. To standardize the language and make everything in English, we used Argostranslate. Below is the code we used to translate the 'Author' column from Arabic to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: argostranslate in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (1.9.6)\n",
      "Requirement already satisfied: ctranslate2<5,>=4.0 in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from argostranslate) (4.5.0)\n",
      "Requirement already satisfied: stanza==1.1.1 in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from argostranslate) (1.1.1)\n",
      "Requirement already satisfied: sentencepiece==0.2.0 in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from argostranslate) (0.2.0)\n",
      "Requirement already satisfied: sacremoses==0.0.53 in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from argostranslate) (0.0.53)\n",
      "Requirement already satisfied: packaging in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from argostranslate) (21.3)\n",
      "Requirement already satisfied: click in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from sacremoses==0.0.53->argostranslate) (8.0.4)\n",
      "Requirement already satisfied: regex in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from sacremoses==0.0.53->argostranslate) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from sacremoses==0.0.53->argostranslate) (4.64.0)\n",
      "Requirement already satisfied: six in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from sacremoses==0.0.53->argostranslate) (1.16.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from sacremoses==0.0.53->argostranslate) (1.1.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from stanza==1.1.1->argostranslate) (5.29.3)\n",
      "Requirement already satisfied: requests in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from stanza==1.1.1->argostranslate) (2.27.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from stanza==1.1.1->argostranslate) (1.21.5)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from stanza==1.1.1->argostranslate) (2.6.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from ctranslate2<5,>=4.0->argostranslate) (61.2.0)\n",
      "Requirement already satisfied: pyyaml<7,>=5.3 in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from ctranslate2<5,>=4.0->argostranslate) (6.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza==1.1.1->argostranslate) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza==1.1.1->argostranslate) (1.13.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza==1.1.1->argostranslate) (2.11.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza==1.1.1->argostranslate) (2.7.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza==1.1.1->argostranslate) (4.10.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza==1.1.1->argostranslate) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.3.0->stanza==1.1.1->argostranslate) (1.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from click->sacremoses==0.0.53->argostranslate) (0.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.3.0->stanza==1.1.1->argostranslate) (2.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from packaging->argostranslate) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from requests->stanza==1.1.1->argostranslate) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from requests->stanza==1.1.1->argostranslate) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from requests->stanza==1.1.1->argostranslate) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from requests->stanza==1.1.1->argostranslate) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install argostranslate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df_jarir after translation:\n",
      "               Author\n",
      "0     Walid Al-Anazi\n",
      "1     Adam J. Kertz.\n",
      "2      Imad Al-Zarri\n",
      "3  Robert T. Kiusaki\n",
      "4            Fatima.\n",
      "\n",
      "df_amazon after translation:\n",
      "                                Author\n",
      "0  Nasser bin Abdulaziz Al-Abdulkarim\n",
      "1                       Imad Al-Zarri\n",
      "2                       Jérémy Mariez\n",
      "3                       Jérémy Mariez\n",
      "4                 Wonder House Book s\n"
     ]
    }
   ],
   "source": [
    "import argostranslate.package\n",
    "import argostranslate.translate\n",
    "\n",
    "from_code = \"ar\"\n",
    "to_code = \"en\"\n",
    "\n",
    "argostranslate.package.update_package_index()\n",
    "available_packages = argostranslate.package.get_available_packages()\n",
    "package_to_install = next(\n",
    "    filter(\n",
    "        lambda x: x.from_code == from_code and x.to_code == to_code, available_packages\n",
    "    )\n",
    ")\n",
    "argostranslate.package.install_from_path(package_to_install.download())\n",
    "\n",
    "df_jarir['Author'] = df_jarir['Author'].apply(lambda x: argostranslate.translate.translate(x, from_code, to_code))\n",
    "df_amazon['Author'] = df_amazon['Author'].apply(lambda x: argostranslate.translate.translate(x, from_code, to_code))\n",
    "\n",
    "print(\"\\ndf_jarir after translation:\\n\", df_jarir[['Author']].head())\n",
    "print(\"\\ndf_amazon after translation:\\n\", df_amazon[['Author']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Num Of Reviews</th>\n",
       "      <th>Author</th>\n",
       "      <th>Book Type</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Cover Image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>يقولون الاولين</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00905</td>\n",
       "      <td>Walid Al-Anazi</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Fiction &amp; Literature,, New Arrivals, Best Sell...</td>\n",
       "      <td>https://www.jarir.com/cdn-cgi/image/fit=contai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>صفحة واحدة في المرة رفيق مبدع يومي</td>\n",
       "      <td>3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.00459</td>\n",
       "      <td>Adam J. Kertz.</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Jarir Publications Books,, Self Development,, ...</td>\n",
       "      <td>https://www.jarir.com/cdn-cgi/image/fit=contai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>المعاصر 9 تأسيس كمي ورقمي ومحوسب مع بنك المحوس...</td>\n",
       "      <td>4</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.00360</td>\n",
       "      <td>Imad Al-Zarri</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>School Textbooks,, New Arrivals, Best Sellers,</td>\n",
       "      <td>https://www.jarir.com/cdn-cgi/image/fit=contai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>الاب الغني الاب الفقير ما يعلمه الاثرياء ولا ي...</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>Robert T. Kiusaki</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Jarir Publications Books,, Best Sellers,, Busi...</td>\n",
       "      <td>https://www.jarir.com/cdn-cgi/image/fit=contai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>مصحف القيام مقاس 25×35</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00248</td>\n",
       "      <td>Fatima.</td>\n",
       "      <td>غلاف مقوى فني</td>\n",
       "      <td>Islamic &amp; Religion Books,, New Arrivals,, Best...</td>\n",
       "      <td>https://www.jarir.com/cdn-cgi/image/fit=contai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>The Coworker - You See Her Everyday</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>Freida McFadden</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Fiction &amp; Literature,, Best Sellers,</td>\n",
       "      <td>https://www.jarir.com/cdn-cgi/image/fit=contai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>Psychology of Money - Timeless Lesson on Wealth</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>Morgan Housel</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Business &amp; Management,, Best Sellers</td>\n",
       "      <td>https://www.jarir.com/cdn-cgi/image/fit=contai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>Dune</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>Frank Herbe r t</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Fiction &amp; Literature,, Science Fiction,, Best ...</td>\n",
       "      <td>https://www.jarir.com/cdn-cgi/image/fit=contai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>Rich Dad Poor Dad</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>Robert Kiyosaki</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Business &amp; Management,, Best Sellers,</td>\n",
       "      <td>https://www.jarir.com/cdn-cgi/image/fit=contai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>The Final Gambit</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>Jennifer Lynn Barne s</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Best Sellers,, Fiction &amp; Literature</td>\n",
       "      <td>https://www.jarir.com/cdn-cgi/image/fit=contai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title Price  Rating  \\\n",
       "0                                       يقولون الاولين     3     5.0   \n",
       "1                   صفحة واحدة في المرة رفيق مبدع يومي     3     4.5   \n",
       "2    المعاصر 9 تأسيس كمي ورقمي ومحوسب مع بنك المحوس...     4     4.7   \n",
       "3    الاب الغني الاب الفقير ما يعلمه الاثرياء ولا ي...     4     5.0   \n",
       "4                               مصحف القيام مقاس 25×35     4     5.0   \n",
       "..                                                 ...   ...     ...   \n",
       "256                The Coworker - You See Her Everyday     3     0.0   \n",
       "257    Psychology of Money - Timeless Lesson on Wealth     4     0.0   \n",
       "258                                               Dune     3     5.0   \n",
       "259                                  Rich Dad Poor Dad     3     0.0   \n",
       "260                                   The Final Gambit     3     0.0   \n",
       "\n",
       "     Num Of Reviews                 Author      Book Type  \\\n",
       "0           0.00905         Walid Al-Anazi      Paperback   \n",
       "1           0.00459         Adam J. Kertz.      Paperback   \n",
       "2           0.00360          Imad Al-Zarri      Paperback   \n",
       "3           0.00298      Robert T. Kiusaki      Paperback   \n",
       "4           0.00248                Fatima.  غلاف مقوى فني   \n",
       "..              ...                    ...            ...   \n",
       "256         0.00000        Freida McFadden      Paperback   \n",
       "257         0.00000          Morgan Housel      Paperback   \n",
       "258         0.00012        Frank Herbe r t      Paperback   \n",
       "259         0.00000        Robert Kiyosaki      Paperback   \n",
       "260         0.00000  Jennifer Lynn Barne s      Paperback   \n",
       "\n",
       "                                                 Genre  \\\n",
       "0    Fiction & Literature,, New Arrivals, Best Sell...   \n",
       "1    Jarir Publications Books,, Self Development,, ...   \n",
       "2       School Textbooks,, New Arrivals, Best Sellers,   \n",
       "3    Jarir Publications Books,, Best Sellers,, Busi...   \n",
       "4    Islamic & Religion Books,, New Arrivals,, Best...   \n",
       "..                                                 ...   \n",
       "256               Fiction & Literature,, Best Sellers,   \n",
       "257               Business & Management,, Best Sellers   \n",
       "258  Fiction & Literature,, Science Fiction,, Best ...   \n",
       "259              Business & Management,, Best Sellers,   \n",
       "260                Best Sellers,, Fiction & Literature   \n",
       "\n",
       "                                           Cover Image  \n",
       "0    https://www.jarir.com/cdn-cgi/image/fit=contai...  \n",
       "1    https://www.jarir.com/cdn-cgi/image/fit=contai...  \n",
       "2    https://www.jarir.com/cdn-cgi/image/fit=contai...  \n",
       "3    https://www.jarir.com/cdn-cgi/image/fit=contai...  \n",
       "4    https://www.jarir.com/cdn-cgi/image/fit=contai...  \n",
       "..                                                 ...  \n",
       "256  https://www.jarir.com/cdn-cgi/image/fit=contai...  \n",
       "257  https://www.jarir.com/cdn-cgi/image/fit=contai...  \n",
       "258  https://www.jarir.com/cdn-cgi/image/fit=contai...  \n",
       "259  https://www.jarir.com/cdn-cgi/image/fit=contai...  \n",
       "260  https://www.jarir.com/cdn-cgi/image/fit=contai...  \n",
       "\n",
       "[261 rows x 8 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jarir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Num Of Reviews</th>\n",
       "      <th>Author</th>\n",
       "      <th>Book Type</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Cover Image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>كتاب التحصيلي علمي 46-47 (2025)</td>\n",
       "      <td>4</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.00099</td>\n",
       "      <td>Nasser bin Abdulaziz Al-Abdulkarim</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Graduate School Test Guides</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El Sharq library المعاصر 9 تاسيس كمي 2/1 ورقي ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.02703</td>\n",
       "      <td>Imad Al-Zarri</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Graduate School Test Guides</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coloriages mystères Pixar</td>\n",
       "      <td>4</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.32279</td>\n",
       "      <td>Jérémy Mariez</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Self-Help</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coloriages mystères Disney Princesses: Colorie...</td>\n",
       "      <td>4</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.72507</td>\n",
       "      <td>Jérémy Mariez</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Sports &amp; Outdoors</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My First 365 Coloring Book: Jumbo Coloring Boo...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.20064</td>\n",
       "      <td>Wonder House Book s</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Literature &amp; Fiction for Children</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Crime and Punishment</td>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.33779</td>\n",
       "      <td>Fyodor Dostoyevsky</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Mysteries</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>The Ballad of Never After: the stunning sequel...</td>\n",
       "      <td>3</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.32477</td>\n",
       "      <td>Stephanie Garbe r</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Romantic Fantasy</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>No Longer Human</td>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.69343</td>\n",
       "      <td>Osamu Dazai</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Historical Fiction</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>The Coming Wave: Technology, Power, and the Tw...</td>\n",
       "      <td>3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.10962</td>\n",
       "      <td>Mustafa Suleyman</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Technology</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>IKIGAI</td>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>5.22402</td>\n",
       "      <td>Héctor García</td>\n",
       "      <td>Hardcover</td>\n",
       "      <td>Self-Help for Anger Management</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title Price  Rating  \\\n",
       "0                     كتاب التحصيلي علمي 46-47 (2025)     4     4.3   \n",
       "1   El Sharq library المعاصر 9 تاسيس كمي 2/1 ورقي ...     4     4.5   \n",
       "2                           Coloriages mystères Pixar     4     4.7   \n",
       "3   Coloriages mystères Disney Princesses: Colorie...     4     4.7   \n",
       "4   My First 365 Coloring Book: Jumbo Coloring Boo...     1     4.6   \n",
       "..                                                ...   ...     ...   \n",
       "75                               Crime and Punishment     3     4.6   \n",
       "76  The Ballad of Never After: the stunning sequel...     3     4.8   \n",
       "77                                    No Longer Human     3     4.7   \n",
       "78  The Coming Wave: Technology, Power, and the Tw...     3     4.4   \n",
       "79                                             IKIGAI     4     4.6   \n",
       "\n",
       "    Num Of Reviews                              Author  Book Type  \\\n",
       "0          0.00099  Nasser bin Abdulaziz Al-Abdulkarim  Paperback   \n",
       "1          0.02703                       Imad Al-Zarri  Paperback   \n",
       "2          0.32279                       Jérémy Mariez  Paperback   \n",
       "3          0.72507                       Jérémy Mariez  Paperback   \n",
       "4          0.20064                 Wonder House Book s  Paperback   \n",
       "..             ...                                 ...        ...   \n",
       "75         0.33779                  Fyodor Dostoyevsky  Paperback   \n",
       "76         0.32477                   Stephanie Garbe r  Paperback   \n",
       "77         1.69343                         Osamu Dazai  Paperback   \n",
       "78         0.10962                    Mustafa Suleyman  Paperback   \n",
       "79         5.22402                       Héctor García  Hardcover   \n",
       "\n",
       "                                Genre  \\\n",
       "0         Graduate School Test Guides   \n",
       "1         Graduate School Test Guides   \n",
       "2                           Self-Help   \n",
       "3                   Sports & Outdoors   \n",
       "4   Literature & Fiction for Children   \n",
       "..                                ...   \n",
       "75                          Mysteries   \n",
       "76                   Romantic Fantasy   \n",
       "77                 Historical Fiction   \n",
       "78                         Technology   \n",
       "79     Self-Help for Anger Management   \n",
       "\n",
       "                                          Cover Image  \n",
       "0   https://images-eu.ssl-images-amazon.com/images...  \n",
       "1   https://images-eu.ssl-images-amazon.com/images...  \n",
       "2   https://images-eu.ssl-images-amazon.com/images...  \n",
       "3   https://images-eu.ssl-images-amazon.com/images...  \n",
       "4   https://images-eu.ssl-images-amazon.com/images...  \n",
       "..                                                ...  \n",
       "75  https://images-eu.ssl-images-amazon.com/images...  \n",
       "76  https://images-eu.ssl-images-amazon.com/images...  \n",
       "77  https://images-eu.ssl-images-amazon.com/images...  \n",
       "78  https://images-eu.ssl-images-amazon.com/images...  \n",
       "79  https://images-eu.ssl-images-amazon.com/images...  \n",
       "\n",
       "[79 rows x 8 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amazon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genre Cleaning and Standardization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_genre_column(df):\n",
    "    df['Genre'] = df['Genre'].str.replace(',,', ',').str.strip()\n",
    "\n",
    "    unnecessary_genres = ['Best Sellers', 'Jarir Publications Books', 'New Arrivals', 'Bargain Books', 'E-Books', 'Audio & Video CD/DVD']\n",
    "    for unwanted in unnecessary_genres:\n",
    "        df['Genre'] = df['Genre'].str.replace(f'{unwanted}, ', '', regex=False)\n",
    "        df['Genre'] = df['Genre'].str.replace(f', {unwanted}', '', regex=False)\n",
    "        df['Genre'] = df['Genre'].str.replace(unwanted, '', regex=False).str.replace(',,', ',').str.strip(', ')\n",
    "\n",
    "    return df\n",
    "\n",
    "df_jarir = clean_genre_column(df_jarir)\n",
    "df_amazon = clean_genre_column(df_amazon)\n",
    "\n",
    "genre_mapping = {\n",
    "    'Fiction & Literature': 'Fiction Genres',\n",
    "    'Literary Fiction': 'Fiction Genres',\n",
    "    'Genre Fiction': 'Fiction Genres',\n",
    "    'Mysteries': 'Mystery & Thriller',\n",
    "    'Historical Mystery': 'Mystery & Thriller',\n",
    "    'Psychological Thrillers': 'Mystery & Thriller',\n",
    "    'Science Fiction': 'Science Fiction & Fantasy',\n",
    "    'Romantic Fantasy': 'Science Fiction & Fantasy',\n",
    "    'Psychological Fiction': 'Psychological & Drama',\n",
    "    'Comics & Graphic Novels': 'Comics & Humor',\n",
    "    'Manga Comics & Graphic Novels': 'Comics & Humor',\n",
    "    'Humor': 'Comics & Humor',\n",
    "    'Humorous Fiction': 'Comics & Humor',\n",
    "    'Self-Help': 'Self-Help & Personal Development',\n",
    "    'Self Development': 'Self-Help & Personal Development',\n",
    "    'Self-Esteem': 'Self-Help & Personal Development',\n",
    "    'Motivational Self-Help': 'Self-Help & Personal Development',\n",
    "    'Health': 'Health & Fitness',\n",
    "    'Fitness & Dieting': 'Health & Fitness',\n",
    "    'Diets': 'Health & Fitness',\n",
    "    'Business & Management': 'Business & Finance',\n",
    "    'Business Management': 'Business & Finance',\n",
    "    'Finance': 'Business & Finance',\n",
    "    'Forensic Psychology': 'Psychology & Philosophy',\n",
    "    'Psychology & Counseling': 'Psychology & Philosophy',\n",
    "    'Philosophy': 'Psychology & Philosophy',\n",
    "    'Politics & Social Sciences': 'Politics & Social Sciences',\n",
    "    'Graduate School Test Guides': 'Education & Reference',\n",
    "    'School Textbooks': 'Education & Reference',\n",
    "    'Higher & Continuing Education': 'Education & Reference',\n",
    "    'Foreign Language Study & Reference': 'Education & Reference',\n",
    "    'Textbooks and Study Guides': 'Education & Reference',\n",
    "    'Technology': 'Technology & Digital Media',\n",
    "    'Children\\'s Early Learning Books on the Alphabet': 'Children & Young Adults',\n",
    "    'Early Learning for Children': 'Children & Young Adults',\n",
    "    'Explore the World Books for Children': 'Children & Young Adults',\n",
    "    'Children Books': 'Children & Young Adults',\n",
    "    'Education Studies & Teaching': 'Children & Young Adults',\n",
    "    'Children\\'s Books on Country & Farm Life': 'Children & Young Adults',\n",
    "    'Children\\'s Books on Family Life': 'Children & Young Adults',\n",
    "    'Children\\'s Books on Social Skills': 'Children & Young Adults',\n",
    "    'Children\\'s Books on Crafts & Hobbies': 'Children & Young Adults',\n",
    "    'Literature & Fiction for Children': 'Children & Young Adults',\n",
    "    'Fantasy for Young Adults': 'Children & Young Adults',\n",
    "    'Mysteries & Thrillers for Young Adults': 'Children & Young Adults',\n",
    "    'Digital Art': 'Children & Young Adults',\n",
    "    'Drawing': 'Arts, Crafts, & Hobbies',\n",
    "    'Crafts': 'Arts, Crafts, & Hobbies',\n",
    "    'Crafts & Hobbies': 'Arts, Crafts, & Hobbies',\n",
    "    'Hobbies & Home': 'Arts, Crafts, & Hobbies',\n",
    "    'Sports & Outdoors': 'Arts, Crafts, & Hobbies',\n",
    "    'Islamic & Religion Books': 'Religion & Spirituality',\n",
    "    'Islam': 'Religion & Spirituality',\n",
    "    'History': 'History & Culture',\n",
    "    'History & Geography': 'History & Culture',\n",
    "    'Antiques & Collectibles': 'History & Culture',\n",
    "    'Poetry & Literary Essays': 'Poetry & Essays',\n",
    "    'Poetry Anthologies': 'Poetry & Essays',\n",
    "    'Literary Essays & Correspondence': 'Poetry & Essays',\n",
    "    'Family & Relationships': 'Parenting & Family',\n",
    "    'Parenting & Family': 'Parenting & Family',\n",
    "    'Cooking & Culinary Arts': 'Cookbooks & Food',\n",
    "    'Celebrity & TV Show Cookbooks': 'Cookbooks & Food',\n",
    "    'Baking': 'Cookbooks & Food',\n",
    "}\n",
    "\n",
    "def clean_genre_list(genre_str):\n",
    "    if pd.isna(genre_str):  # Check if the value is NaN or None\n",
    "        return None\n",
    "\n",
    "    genres = [g.strip() for g in genre_str.split(',') if g.strip()]\n",
    "\n",
    "    mapped_genres = set(genre_mapping.get(g, g) for g in genres)\n",
    "\n",
    "    main_categories_priority = ['Fiction Genres', 'Non-Fiction Genres', 'Children & Young Adults', 'Arts, Crafts, & Hobbies',\n",
    "                                'Religion & Spirituality', 'History & Culture', 'Poetry & Essays', 'Parenting & Family', 'Cookbooks & Food']\n",
    "    for category in main_categories_priority:\n",
    "        if category in mapped_genres:\n",
    "            return category\n",
    "    return ', '.join(mapped_genres) if mapped_genres else None\n",
    "\n",
    "df_jarir['Genre'] = df_jarir['Genre'].apply(clean_genre_list)\n",
    "df_amazon['Genre'] = df_amazon['Genre'].apply(clean_genre_list)\n",
    "\n",
    "df_jarir.dropna(subset=['Genre'], inplace=True)\n",
    "df_amazon.dropna(subset=['Genre'], inplace=True)\n",
    "df_jarir.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "#rawBooks_df.to_csv(\"working.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section, we focused on cleaning and standardizing the `Genre` column to ensure consistency and remove redundant or irrelevant entries. We started by eliminating duplicate commas and unnecessary whitespace. Then, we removed irrelevant values such as \"Best Sellers\" and \"Audio & Video CD/DVD.\" Finally, we applied a mapping to unify similar genres under standardized names, for example, \"Fiction & Literature\" was categorized as \"Fiction Genres.\" This step was crucial to maintain a consistent data structure and improve the quality of analysis based on genre classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In addition, we want to mention that we didn’t perform preprocessing on the 'Book Type' column. Although we know it contains different languages and some issues, we decided not to spend time on it for now. If we later determine that it’s necessary, we will apply the proper preprocessing. Otherwise, we will drop the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jarir.to_csv(\"Clean_Jarir.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon.to_csv(\"Clean_Amazon.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Book Cover Images Using Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.27.1)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from webdriver-manager) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from packaging->webdriver-manager) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rikhm\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2025.1.31)\n",
      "Installing collected packages: python-dotenv, webdriver-manager\n",
      "Successfully installed python-dotenv-1.0.1 webdriver-manager-4.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image download process completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "jarir_file_path = \"Clean_Jarir.csv\"  # Ensure this file exists\n",
    "amazon_file_path = \"Clean_Amazon.csv\"  # Ensure this file exists\n",
    "\n",
    "df_jarir = pd.read_csv(jarir_file_path)\n",
    "df_amazon = pd.read_csv(amazon_file_path)\n",
    "\n",
    "df = pd.concat([df_jarir, df_amazon], ignore_index=True)\n",
    "\n",
    "# Set up Selenium options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "# Start WebDriver\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Directory to save images\n",
    "image_dir = \"book_covers\"\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "# Number of images to download\n",
    "num_images = 315  # Adjust as needed\n",
    "\n",
    "# Extract image URLs from the \"Cover Image\" column\n",
    "image_urls = df[\"Cover Image\"].dropna().head(num_images).tolist()\n",
    "\n",
    "# Download images using Selenium and save them directly\n",
    "for idx, url in enumerate(image_urls):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # Wait for the page to load\n",
    "        \n",
    "        # Locate the image element\n",
    "        image_element = driver.find_element(\"tag name\", \"img\")\n",
    "\n",
    "        # Save the image via Selenium\n",
    "        image_path = os.path.join(image_dir, f\"book_{idx}.png\")\n",
    "        image_element.screenshot(image_path)\n",
    "\n",
    "    except Exception as e :\n",
    "        print(f\"Error downloading image {idx}: {e}\")  # Print the error message\n",
    "        pass  # Ignore errors and continue\n",
    "\n",
    "# Close the browser after completing the task\n",
    "driver.quit()\n",
    "print(\"Image download process completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used Selenium to download book cover images from processed_data.csv, bypassing Cloudflare protection and dynamic loading by simulating real browsing. Running Selenium in headless mode, we stored images in book_covers, extracting 313 URLs from the \"Cover Image\" column. For each, we opened the webpage, located the `<img>` element, and captured a screenshot. Error handling ensured smooth execution. With the covers downloaded, we can now analyze them for classification using color extraction, contrast analysis, or machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Analyzing Images and Extracting Key Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will load the downloaded images and extract the following visual features [ 4 ] :\n",
    "\n",
    "- Mean Color: To determine the dominant colors in each cover.\n",
    "- Contrast: To measure how clear and vibrant the cover is.\n",
    "- Edge Complexity: To assess the level of detail and design complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# Define the directory where book cover images are stored\n",
    "image_dir = \"book_covers\"\n",
    "image_files = [f for f in os.listdir(image_dir) if f.endswith(\".png\")]\n",
    "\n",
    "# Function to extract visual features from images\n",
    "def extract_visual_features(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Compute mean color\n",
    "    mean_color = np.mean(img, axis=(0, 1))\n",
    "\n",
    "    # Convert the image to grayscale for contrast analysis\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    contrast = np.std(gray_img)\n",
    "\n",
    "    # Use edge detection to measure design complexity\n",
    "    edges = cv2.Canny(gray_img, 100, 200)\n",
    "    edge_complexity = np.mean(edges)\n",
    "\n",
    "    return mean_color, contrast, edge_complexity\n",
    "\n",
    "# Extract features from all images\n",
    "features = []\n",
    "for img_file in image_files:\n",
    "    img_path = os.path.join(image_dir, img_file)\n",
    "    mean_color, contrast, edge_complexity = extract_visual_features(img_path)\n",
    "    features.append((img_file, mean_color, contrast, edge_complexity))\n",
    "\n",
    "# Convert the extracted features into a DataFrame\n",
    "df_features = pd.DataFrame(features, columns=[\"Filename\", \"Mean_Color\", \"Contrast\", \"Edge_Complexity\"])\n",
    "\n",
    "# Save extracted features to a CSV file\n",
    "df_features.to_csv(\"book_cover_features.csv\", index=False)\n",
    "\n",
    "# Load extracted feature data\n",
    "df_features = pd.read_csv(\"book_cover_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Covers as \"Interesting\" or \"Not Interesting\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting features, we will classify covers as \"Interesting\" if they exceed a threshold in contrast or edge complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate thresholds for determining interesting covers\n",
    "contrast_threshold = df_features[\"Contrast\"].median()\n",
    "edge_threshold = df_features[\"Edge_Complexity\"].median()\n",
    "\n",
    "# Adjust the threshold slightly to make classification more practical\n",
    "contrast_threshold *= 0.8\n",
    "edge_threshold *= 0.8\n",
    "\n",
    "# Classify covers based on extracted features\n",
    "df_features[\"Interesting\"] = df_features.apply(\n",
    "    lambda row: 1 if row[\"Contrast\"] > contrast_threshold or row[\"Edge_Complexity\"] > edge_threshold else 0, axis=1\n",
    ")\n",
    "\n",
    "# Save the classified covers to a new CSV file\n",
    "df_features.to_csv(\"book_cover_features_with_classification.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Classification Results with Original Book Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will update processed_data.csv to include the new \"Interesting or Not\" classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "jarir_image = pd.read_csv(\"Clean_Jarir.csv\")\n",
    "amazon_image = pd.read_csv(\"Clean_Amazon.csv\")\n",
    "df_features = pd.read_csv(\"book_cover_features_with_classification.csv\")\n",
    "\n",
    "# Standardize 'Book_number' column in df_features\n",
    "df_features[\"Filename\"] = df_features[\"Filename\"].str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "# Ensure df_processed has Book_number column matching df_features\n",
    "jarir_image[\"Book_number\"] = range(0, len(df_jarir))\n",
    "amazon_image[\"Book_number\"] = range(236, 236 + len(amazon_image))\n",
    "\n",
    "# Merge based on Book_number\n",
    "jarir_image = jarir_image.merge(df_features[[\"Filename\", \"Interesting\"]], left_on=\"Book_number\", right_on=\"Filename\" ,how=\"left\")\n",
    "amazon_image = amazon_image.merge(df_features[[\"Filename\", \"Interesting\"]], left_on=\"Book_number\", right_on=\"Filename\" ,how=\"left\")\n",
    "\n",
    "jarir_image.drop(columns=[\"Filename\", \"Book_number\"], inplace=True)\n",
    "amazon_image.drop(columns=[\"Filename\", \"Book_number\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 236 entries, 0 to 235\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Title           236 non-null    object \n",
      " 1   Price           236 non-null    int64  \n",
      " 2   Rating          236 non-null    float64\n",
      " 3   Num Of Reviews  236 non-null    float64\n",
      " 4   Author          236 non-null    object \n",
      " 5   Book Type       236 non-null    object \n",
      " 6   Genre           236 non-null    object \n",
      " 7   Cover Image     236 non-null    object \n",
      " 8   Interesting     236 non-null    int64  \n",
      "dtypes: float64(2), int64(2), object(5)\n",
      "memory usage: 18.4+ KB\n"
     ]
    }
   ],
   "source": [
    "jarir_image.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 79 entries, 0 to 78\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Title           79 non-null     object \n",
      " 1   Price           79 non-null     int64  \n",
      " 2   Rating          79 non-null     float64\n",
      " 3   Num Of Reviews  79 non-null     float64\n",
      " 4   Author          79 non-null     object \n",
      " 5   Book Type       79 non-null     object \n",
      " 6   Genre           79 non-null     object \n",
      " 7   Cover Image     79 non-null     object \n",
      " 8   Interesting     79 non-null     int64  \n",
      "dtypes: float64(2), int64(2), object(5)\n",
      "memory usage: 6.2+ KB\n"
     ]
    }
   ],
   "source": [
    "amazon_image.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Processed into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Processed into a csv file\n",
    "jarir_image.to_csv(\"processed_jarir.csv\", index=False)\n",
    "amazon_image.to_csv(\"processed_amazon.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Advanced Exploratory Data Analysis ( EDA )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing and cleaning our data , we have to analyze , summarize and visualize it to understand their main characteristics using EDA.\n",
    "\n",
    "This analysis is based on both primary and secondary data collected through web scraping. Data from Amazon serves as the primary source, while data from Jarir has been used as a secondary source to enrich and enhance the dataset. The inclusion of Jarir data allows for a more comprehensive and balanced analysis, providing additional insights through web scraping techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading processed data \n",
    "processed_amazon = pd.read_csv(\"processed_amazon.csv\")\n",
    "processed_jarir = pd.read_csv(\"processed_jarir.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Non-graphical , Univariate : statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">Amazon Statistics</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Jarir Statistics</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Num Of Reviews</th>\n",
       "      <th>Interesting</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Num Of Reviews</th>\n",
       "      <th>Interesting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>79.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.772152</td>\n",
       "      <td>4.578481</td>\n",
       "      <td>1.125657</td>\n",
       "      <td>0.848101</td>\n",
       "      <td>2.008475</td>\n",
       "      <td>2.904237</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.894068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.348783</td>\n",
       "      <td>0.540102</td>\n",
       "      <td>2.096440</td>\n",
       "      <td>0.361216</td>\n",
       "      <td>1.247948</td>\n",
       "      <td>2.385966</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.308405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.550000</td>\n",
       "      <td>0.048855</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.314360</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>1.151335</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.011280</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Amazon Statistics                                        \\\n",
       "                  Price     Rating Num Of Reviews Interesting   \n",
       "count         79.000000  79.000000      79.000000   79.000000   \n",
       "mean           2.772152   4.578481       1.125657    0.848101   \n",
       "std            1.348783   0.540102       2.096440    0.361216   \n",
       "min            0.000000   0.000000       0.000000    0.000000   \n",
       "25%            2.000000   4.550000       0.048855    1.000000   \n",
       "50%            3.000000   4.600000       0.314360    1.000000   \n",
       "75%            4.000000   4.700000       1.151335    1.000000   \n",
       "max            4.000000   4.900000      10.000000    1.000000   \n",
       "\n",
       "      Jarir Statistics                                         \n",
       "                 Price      Rating Num Of Reviews Interesting  \n",
       "count       236.000000  236.000000     236.000000  236.000000  \n",
       "mean          2.008475    2.904237       0.000402    0.894068  \n",
       "std           1.247948    2.385966       0.001097    0.308405  \n",
       "min           0.000000    0.000000       0.000000    0.000000  \n",
       "25%           1.000000    0.000000       0.000000    1.000000  \n",
       "50%           2.000000    4.600000       0.000120    1.000000  \n",
       "75%           3.000000    5.000000       0.000280    1.000000  \n",
       "max           4.000000    5.000000       0.011280    1.000000  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([processed_amazon.describe(), processed_jarir.describe()], axis=1, keys=[\"Amazon Statistics\", \"Jarir Statistics\"]) # for numeric attributes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain a brief overview of the statistical properties of numerical attributes, we can use the `describe()` function, which provides insights such as the mean, minimum, maximum, and quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">Amazon Non-Numeric</th>\n",
       "      <th colspan=\"5\" halign=\"left\">Jarir Non-Numeric</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Book Type</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Cover Image</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Book Type</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Cover Image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>236</td>\n",
       "      <td>236</td>\n",
       "      <td>236</td>\n",
       "      <td>236</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>79</td>\n",
       "      <td>67</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>79</td>\n",
       "      <td>234</td>\n",
       "      <td>125</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>كتاب التحصيلي علمي 46-47 (2025)</td>\n",
       "      <td>Jérémy Mariez</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Children &amp; Young Adults</td>\n",
       "      <td>https://images-eu.ssl-images-amazon.com/images...</td>\n",
       "      <td>مميز بالاصفر</td>\n",
       "      <td>Greer's library.</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>Children &amp; Young Adults</td>\n",
       "      <td>https://www.jarir.com/cdn-cgi/image/fit=contai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>55</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>104</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Amazon Non-Numeric                            \\\n",
       "                                  Title         Author  Book Type   \n",
       "count                                79             79         79   \n",
       "unique                               79             67          5   \n",
       "top     كتاب التحصيلي علمي 46-47 (2025)  Jérémy Mariez  Paperback   \n",
       "freq                                  1              5         55   \n",
       "\n",
       "                                 \\\n",
       "                          Genre   \n",
       "count                        79   \n",
       "unique                       26   \n",
       "top     Children & Young Adults   \n",
       "freq                         22   \n",
       "\n",
       "                                                          Jarir Non-Numeric  \\\n",
       "                                              Cover Image             Title   \n",
       "count                                                  79               236   \n",
       "unique                                                 79               234   \n",
       "top     https://images-eu.ssl-images-amazon.com/images...      مميز بالاصفر   \n",
       "freq                                                    1                 2   \n",
       "\n",
       "                                                              \\\n",
       "                  Author  Book Type                    Genre   \n",
       "count                236        236                      236   \n",
       "unique               125          5                       10   \n",
       "top     Greer's library.  Paperback  Children & Young Adults   \n",
       "freq                  24        104                      126   \n",
       "\n",
       "                                                           \n",
       "                                              Cover Image  \n",
       "count                                                 236  \n",
       "unique                                                236  \n",
       "top     https://www.jarir.com/cdn-cgi/image/fit=contai...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([processed_amazon.describe(exclude=\"number\"), processed_jarir.describe(exclude=\"number\")], axis=1, keys=[\"Amazon Non-Numeric\", \"Jarir Non-Numeric\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For non-numerical attributes, we can use `describe(exclude=\"number\")` to gain valuable insights. From this, we can derive the following points:\n",
    "\n",
    "- For Amazon, out of 79 books, there are 67 unique `Author`, with `Jérémy Mariez` being the most frequently mentioned, appearing in 5 books. On the other hand, for Jarir, out of 236 books, there are 125 unique `Author`, with `Greer's Library` being the most frequently mentioned, appearing in 24 books .\n",
    "- The dataset includes 24 distinct genres, with 147 books categorized under `Children & Young Adults` genre.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Non-graphical , Multivariate : correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">Amazon Correlation</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Jarir Correlation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Num Of Reviews</th>\n",
       "      <th>Interesting</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Num Of Reviews</th>\n",
       "      <th>Interesting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.077213</td>\n",
       "      <td>0.060265</td>\n",
       "      <td>-0.177207</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.226934</td>\n",
       "      <td>0.186971</td>\n",
       "      <td>-0.030827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rating</th>\n",
       "      <td>-0.077213</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.127249</td>\n",
       "      <td>-0.036684</td>\n",
       "      <td>0.226934</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.293149</td>\n",
       "      <td>-0.140490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Num Of Reviews</th>\n",
       "      <td>0.060265</td>\n",
       "      <td>0.127249</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.071460</td>\n",
       "      <td>0.186971</td>\n",
       "      <td>0.293149</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.069912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interesting</th>\n",
       "      <td>-0.177207</td>\n",
       "      <td>-0.036684</td>\n",
       "      <td>-0.071460</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.030827</td>\n",
       "      <td>-0.140490</td>\n",
       "      <td>-0.069912</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Amazon Correlation                                       \\\n",
       "                            Price    Rating Num Of Reviews Interesting   \n",
       "Price                    1.000000 -0.077213       0.060265   -0.177207   \n",
       "Rating                  -0.077213  1.000000       0.127249   -0.036684   \n",
       "Num Of Reviews           0.060265  0.127249       1.000000   -0.071460   \n",
       "Interesting             -0.177207 -0.036684      -0.071460    1.000000   \n",
       "\n",
       "               Jarir Correlation                                       \n",
       "                           Price    Rating Num Of Reviews Interesting  \n",
       "Price                   1.000000  0.226934       0.186971   -0.030827  \n",
       "Rating                  0.226934  1.000000       0.293149   -0.140490  \n",
       "Num Of Reviews          0.186971  0.293149       1.000000   -0.069912  \n",
       "Interesting            -0.030827 -0.140490      -0.069912    1.000000  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([processed_amazon.corr(), processed_jarir.corr()], axis=1, keys=[\"Amazon Correlation\", \"Jarir Correlation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we can conclude:\n",
    "- Amazon's `Rating` has a weak positive correlation with the `Num Of Reviews` 0.127249 . \n",
    "- Jarir's `Rating` has a stronger positive correlation with the `Num Of Reviews` 0.293149 .\n",
    "- Amazon's price has a weak negative correlation with rating (-0.077213) and a weak positive correlation with the number of reviews (0.060265)\n",
    "- Jarir's price has a weak positive correlation with rating (0.226934) and the number of reviews (0.186971)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Amazon:**\n",
    "\n",
    "- `Rating` has a weak positive correlation with `Num Of Reviews` 0.127 , suggesting that higher-rated products tend to have slightly more reviews.\n",
    "\n",
    "- `Price` has a weak negative correlation with `Rating` -0.077 , indicating that higher-priced items might have slightly lower ratings.\n",
    "\n",
    "- `Price` also has a weak positive correlation with `Num Of Reviews` 0.060 , meaning higher-priced items might have slightly more reviews.\n",
    "\n",
    "**Jarir:**\n",
    "\n",
    "- `Rating` has a stronger positive correlation with `Num Of Reviews` 0.293 , showing that higher-rated products are more likely to have more reviews compared to Amazon.\n",
    "\n",
    "- `Price` has a weak positive correlation with `Rating` 0.227 , suggesting that higher-priced items might have slightly higher ratings.\n",
    "\n",
    "- `Price` also has a weak positive correlation with `Num Of Reviews` 0.187 , meaning higher-priced items might have slightly more reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Graphical , Univariate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Graphical , Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A. Alharbi, \"Exploring Factors Influencing the Amazon Best-Selling Books Selection Process from 2009 to 2019,\" ResearchGate, 2024. [Online]. Available: https://www.researchgate.net/publication/382998978_Exploring_Factors_Influencing_the_Amazon_Best-Selling_Books_Selection_Process_from_2009_to_2019.\n",
    "\n",
    "2. J. Smith and J. Doe, \"Using Full-Text Content to Characterize and Identify Best Seller Books,\" PLOS ONE, May 11, 2023. [Online]. Available: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0302070.\n",
    "\n",
    "3. L. Johnson and K. Brown, \"Analyzing Social Book Reading Behavior on Goodreads and How It Predicts Amazon Best Sellers,\" ResearchGate, 2018. [Online]. Available: https://www.researchgate.net/publication/327789907_Analyzing_Social_Book_Reading_Behavior_on_Goodreads_and_how_it_predicts_Amazon_Best_Sellers.\n",
    "4. Hurix Digital, \"Knowing the Psychology of Book Cover Design,\" Hurix Blogs, 2023. [Online]. Available: [https://www.hurix.com/blogs/knowing-the-psychology-of-book-cover-design](https://www.hurix.com/blogs/knowing-the-psychology-of-book-cover-design)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
